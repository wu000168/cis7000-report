\section{Evaluation}

To evaluate FFL's impact on the experience of authoring augmentations, we conducted an in-lab usability study. The study was designed to answer the following questions:

\begin{enumerate}
\item How does FFL influence authors' ability to create and edit augmentations?
\item How could tools like FFL be improved to better support formula augmentation?
\end{enumerate}

The study consisted of a controlled comparison between FFL and a LaTeX baseline for augmentation creation and editing tasks, followed by an exploratory authoring task with FFL.

\subsection{Participants}\label{study-participants}

We sought participants with experience authoring math documents with LaTeX. Participants were recruited from graduate student mailing lists at a computer science program at a private university, with the sole prerequisite of prior experience writing LaTeX formulas.

33 participants were recruited in total. The vast majority were master's students; 7 were students in a joint bachelor's / master's program. 3 described themselves as software developers, 1 as an academic researcher, and 1 as a teacher.

\zed{Participants' prior experience with LaTeX was as follows}: 24\% reported less than 1 year of experience; 48\% 1--2 years, 21\% 3--5 years, and 6\% reported more than 5 years. 55\% used LaTeX weekly, 18\% monthly, and 24\% less than monthly. Participants reported their comfort with LaTeX as a median of 4 on a 5-point Likert scale ($\sigma = 0.8$, \zed{$\text{IQR}=1$}). They were considerably less comfortable with CSS, with a median comfort level of 2 out of 5 ($\sigma = 1.0$\zed{, $\text{IQR}=1$}).

\subsection{Procedure}

\subsubsection{Setup}
All study sessions were conducted in person in an HCI usability study lab. Participants completed tasks using a computer with a large external monitor, keyboard, and USB mouse. Progress was managed by a custom web app we built to facilitate the study. This app opened the user interfaces participants were expected to use for tasks, and pre-loaded them with task stimuli. It also opened questionnaires after each task. For FFL tasks, participants used a custom live editing environment. For LaTeX tasks, they used Overleaf~\cite{tool:overleaf}. Two participants needed to complete the tasks on a personal laptop instead of the lab computer; these participants' data were used in our qualitative analysis but omitted from the quantitative analysis (Section~\ref{Analysis}).

\subsubsection{Tutorial}
Participants were given 10-minute tutorials of how to augment formulas with both of the interfaces under study---FFL and the LaTeX baseline. A member of the research team demonstrated how to perform key augmentation actions, like selecting expressions, coloring them, and labeling them with line and extent labels, both above and below the formula. Tutorial materials were designed to maximize parity in how the interfaces were introduced while minimizing complexity of the learning material. Participants were asked to practice each feature that was introduced on a sample formula. They were provided with a cheat sheet for each interface to use as a reference during the tasks.

\subsubsection{Interfaces}
The two interfaces participants used were a live editor with FFL support, and a baseline LaTeX environment. \zed{The FFL interface is the same as the environment described in Section~\ref{sec:live_evaluation}. The interface provides only basic support for error recovery: when an author enters invalid FFL, the interface reports that an error was found (without any character positions), while continuing to show the render of the most recent valid FFL}. In LaTeX, participants were taught how to create augmentations using \texttt{\textbackslash textcolor} to color expressions, \texttt{\textbackslash overbrace} or \texttt{\textbackslash underbrace} to introduce labels with extent markers, and \texttt{annotate-equations}~\cite{tool:annotateequations} to introduce labels with leader lines, including the optional argument \texttt{yshift} for adjusting the vertical position of labels.

\subsubsection{Tasks}
Each participant completed four timed tasks and a single exploratory task. After each task, participants completed a questionnaire reflecting on their experience.

\paragraph{Timed tasks}
Participants completed four timed tasks, in two pairs. The first pair of tasks was C1 and C2, which were ``creation'' tasks. In these tasks, participants created augmentations for an unaugmented formula to match a provided screenshot. Each task required participants to add 3 colors and 3 extent labels. 

The second pair of tasks was E1 and E2, which were ``editing'' tasks. In these tasks, participants were given a formula that was already augmented and asked to modify 4 aspects of the augmentation to match a provided screenshot. This latter pair of tasks was designed to reflect the setting where authors need to interact with augmentation markup when evolving their designs.

Within each pair of tasks, participants completed one task with FFL and one task with the LaTeX baseline. Within pairs, tasks were designed to be as similar to each other in difficulty as possible. Participants were randomly assigned interface and task order within each group of tasks, with the following variations, counterbalancing to reduce the effect of task or interface order:

\begin{center}
    \begin{tabular}{r l|r l|r l|r l}
         \multicolumn{2}{c|}{Task 1} & \multicolumn{2}{c|}{Task 2} & \multicolumn{2}{c|}{Task 3} & \multicolumn{2}{c}{Task 4}\\\hline
         C1 &FFL   & C2 &LaTeX & E1 &FFL   & E2 &LaTeX \\
         C2 &FFL   & C1 &LaTeX & E2 &FFL   & E1 &LaTeX \\
         C1 &LaTeX & C2 &FFL   & E1 &LaTeX & E2 & FFL \\
         C2 &LaTeX & C1 &FFL   & E2 &LaTeX & E1 & FFL \\
    \end{tabular}
\end{center}

% which we will refer to as C1, C2, for augmentation \underline{c}reation tasks, and E1, E2 for augmentation \underline{e}diting tasks.

All tasks were timed to compare the speed of completion. A task concluded when a participant completed the task and reported they were done, or when they reached an imposed time limit of 6 minutes and 30 seconds.
\zed{The facilitator verified completion by comparing the participant's output to a reference result using a rubric that permitted very small differences in color and label position. The task duration was chosen by observing that pilot participants completed most tasks within 5 minutes; we then increased task duration to the longest that could be accommodated in the hour-long study.} Over 80\% of tasks were completed before reaching the time limit. 

\paragraph{Exploratory task} Finally, participants were given 10 minutes to augment a short document resembling the one from Section~\ref{Demo}, and asked to augment it in a way that made the formula easier to understand. They were encouraged to explore the augmentation features, and allowed to ask about how to use FFL to achieve their goals. They were also asked to follow the \textit{think-aloud} protocol~\cite{think-aloud}, as demonstrated by their facilitator. 

\subsubsection{Questionnaire and interview instruments}
After each timed task, participants were asked to complete a brief questionnaire reporting how difficult the task was, and to comment on how the interface could have better supported them in their tasks. At the conclusion of the study, participants completed a retrospective questionnaire reflecting on their experience with the interfaces overall. Then, they were interviewed for several minutes as the researcher asked follow-up on questions motivated by observations or responses to the questionnaire.

\subsection{Analysis}\label{Analysis}
To examine the effect of interface on task timing and participants' self-reported ease, we fit them with linear mixed-effects models~\cite{LmmR}. These models take task, task order, and interface and their interactions as fixed effects, and participant as a random effect. Significance was assessed using an F-test using Satterthwaite's estimate of effective degrees of freedom~\cite{Satterthwaite}, with $p$-values corrected by the Holmâ€“Bonferroni method~\cite{Holm}. To compare participants' responses to Likert scale questions about the two interfaces, we performed Wilcoxon signed-rank tests \cite{Wilcoxon}. For these tests, only data from the first 28 of 33 participants was considered, omitting participants who used a personal laptop, and considering a subset for which there was complete balance across interface and task order.

Observation notes, open-ended questionnaire feedback, as well as interview transcripts were analyzed following a thematic analysis approach~\cite{ref:blandford2016qualitative}. Two authors performed an open coding pass, each analyzing half of the observation and questionnaire data and then merging the results. Another two authors reviewed the codes comprehensively. The four authors worked together to revise and organize themes, and to check the alignment between excerpts and themes. One author then reviewed interview transcripts to identify excerpts relating to central themes that emerged from the analysis that had not yet been captured in the observation notes.

